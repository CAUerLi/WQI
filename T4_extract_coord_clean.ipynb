{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 仅仅取每个news_side_id的唯一值\n",
    "##### 5分大约9公里，15分为我27公里，因为坐标过于细致，用2位小数来区分，两位小数约是1公里，三位111m，四位11m\n",
    "- 这样这里需要去掉一些重复坐标点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statisticsData(data, filepath, filename, txtfilename):\n",
    "\n",
    "    unique_sites = data.drop_duplicates(subset='new_site_id', keep='first', inplace=False).reset_index(drop=True)\n",
    "    unique_sites.loc[:, 'lat_wgs84'] = unique_sites['lat_wgs84'].apply(lambda x: f\"{x:.2f}\") # .round(2)\n",
    "    unique_sites.loc[:, 'lon_wgs84'] = unique_sites['lon_wgs84'].apply(lambda x: f\"{x:.2f}\")  # .round(2)\n",
    "    # Group by the rounded 'lat_wgs84' and 'lon_wgs84' and take the first occurrence\n",
    "    unique_sites_grouped = unique_sites.groupby(['lat_wgs84', 'lon_wgs84']).first().reset_index(drop=False)\n",
    "    unique_sites_grouped = unique_sites_grouped.sort_values(by='new_site_id')\n",
    "    unique_sites_grouped['new_new_site_id'] = range(1, len(unique_sites_grouped) + 1)\n",
    "    # Save the unique sites to a new Excel file\n",
    "    unique_sites_grouped.to_csv(os.path.join(filepath, filename + '.csv'), index=True)\n",
    "    # Select the columns to save to the text file\n",
    "    lat_lon_data = unique_sites_grouped[['lat_wgs84', 'lon_wgs84']]\n",
    "    # Save the DataFrame to a text file, without headers and index\n",
    "    lat_lon_data.to_csv(os.path.join(filepath, txtfilename + '.txt'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load WQI file \n",
    "### extract year of 1978-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.chdir(r'F:/WQI20231129')\n",
    "    Inputpath = 'MetaData'\n",
    "    Outputpath = 'MetaData/unique_sites'\n",
    "    filename = 'meta_TN_GRQA.xlsx'\n",
    "    Syear = 1978\n",
    "    Eyear = 2017\n",
    "    OriData = pd.read_excel(os.path.join(Inputpath, filename), engine = 'openpyxl', nrows=500, header=0, index_col=0)\n",
    "    TarData = OriData[(OriData['obs_year'] >= Syear) & (OriData['obs_year'] <= Eyear)].reset_index(drop=True)\n",
    "    # TarData.to_excel('TarData.xlsx', index=True)\n",
    "    statisticsData(TarData, Outputpath, 'unique_sites', 'coordinate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.21seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Time taken: {elapsed_time: .2f}seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WQI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
